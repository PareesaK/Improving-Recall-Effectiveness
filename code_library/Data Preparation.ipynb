{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd1eea92-a546-45e8-b1f3-309a703b9b7a",
   "metadata": {},
   "source": [
    "# FDA Recalls Data Preprocessing\n",
    "**By Lorena Dorado & Parisa Kamizi**\n",
    "- This notebook preprocesses the FDA recall data to create the final dataset for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97f7477c-d4e6-49a5-985a-65fcdc978849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# NLP\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b66a29-ec03-4b3e-9983-12b4fda33316",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8141b2d0-f67e-44bf-bf06-2a2612fa5528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data shape: (95082, 17)\n",
      "\n",
      "Column names:\n",
      "['FEI Number', 'Recalling Firm Name', 'Product Type', 'Product Classification', 'Status', 'Distribution Pattern', 'Recalling Firm City', 'Recalling Firm State', 'Recalling Firm Country', 'Center Classification Date', 'Reason for Recall', 'Product Description', 'Event ID', 'Event Classification', 'Product ID', 'Center', 'Recall Details']\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "file_path = '../data/recalls_details.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Original data shape: {df.shape}\")\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be5b8c10-cb64-4250-9158-07f823df3e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop ID variables\n",
    "drop_ID_cols = [\n",
    "    # ID variables\n",
    "    \"FEI Number\", \"Event ID\", \"Product ID\", \"Recall Details\"]\n",
    "\n",
    "df = df.drop(columns=drop_ID_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93b4b301-c32a-471b-b60a-2f71f4ffa10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values in Distribution Pattern\n",
    "df['Distribution Pattern'] = df['Distribution Pattern'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb0d9499-615d-42b4-8270-0713c22fc30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values per column:\n",
      "Recalling Firm Name           0\n",
      "Product Type                  0\n",
      "Product Classification        0\n",
      "Status                        0\n",
      "Distribution Pattern          0\n",
      "Recalling Firm City           0\n",
      "Recalling Firm State          0\n",
      "Recalling Firm Country        0\n",
      "Center Classification Date    0\n",
      "Reason for Recall             0\n",
      "Product Description           0\n",
      "Event Classification          0\n",
      "Center                        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7dfdb0b-1e17-475b-9964-6469f7a0b863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data shape: (95082, 13)\n",
      "\n",
      "Column names:\n",
      "['Recalling Firm Name', 'Product Type', 'Product Classification', 'Status', 'Distribution Pattern', 'Recalling Firm City', 'Recalling Firm State', 'Recalling Firm Country', 'Center Classification Date', 'Reason for Recall', 'Product Description', 'Event Classification', 'Center']\n"
     ]
    }
   ],
   "source": [
    "# Display cleaned data information\n",
    "print(f\"Cleaned data shape: {df.shape}\")\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f66ce16-e0da-4fe5-ac2f-3ca3ec8e2096",
   "metadata": {},
   "source": [
    "# Data Preparation and Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f08fa225-67b1-4a7a-b585-061ce8d456c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the target variable\n",
    "X = df.drop(columns=['Event Classification'])\n",
    "y = df['Event Classification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "922392e0-a5b7-4939-8dca-c9b1394a85f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (76065, 12)\n",
      "Test set shape: (19017, 12)\n",
      "\n",
      "Training set class distribution: Event Classification\n",
      "Class II     0.708065\n",
      "Class I      0.211516\n",
      "Class III    0.080418\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test set class distribution: Event Classification\n",
      "Class II     0.708103\n",
      "Class I      0.211495\n",
      "Class III    0.080402\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Create a stratified train-test split (80-20)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}\")\n",
    "print(f\"\\nTraining set class distribution: {y_train.value_counts(normalize=True)}\")\n",
    "print(f\"\\nTest set class distribution: {y_test.value_counts(normalize=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b21d12-4a19-43da-a79a-6cd8a7048385",
   "metadata": {},
   "source": [
    "# Feature Engineering on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ddde175-0635-4465-bc7c-0ff17f1268a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy to work with for the training data\n",
    "X_train_processed = X_train.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eda714-1ca7-4b57-a20b-25a2b8d32d04",
   "metadata": {},
   "source": [
    "#### Feature engineering for 'Center Classification Date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8e0196d-f325-4c83-9725-2dd7821b09b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to datetime\n",
    "X_train_processed['Center Classification Date'] = pd.to_datetime(X_train_processed['Center Classification Date'], errors='coerce')\n",
    "\n",
    "# Extract temporal features\n",
    "X_train_processed['Classification Year'] = X_train_processed['Center Classification Date'].dt.year\n",
    "X_train_processed['Classification Month'] = X_train_processed['Center Classification Date'].dt.month\n",
    "X_train_processed['Classification Day'] = X_train_processed['Center Classification Date'].dt.day\n",
    "X_train_processed['Classification DayOfWeek'] = X_train_processed['Center Classification Date'].dt.dayofweek\n",
    "\n",
    "# Add cyclical encoding for month, day, and day of week\n",
    "X_train_processed['Month_sin'] = np.sin(2 * np.pi * X_train_processed['Classification Month']/12)\n",
    "X_train_processed['Month_cos'] = np.cos(2 * np.pi * X_train_processed['Classification Month']/12)\n",
    "X_train_processed['Day_sin'] = np.sin(2 * np.pi * X_train_processed['Classification Day']/31)\n",
    "X_train_processed['Day_cos'] = np.cos(2 * np.pi * X_train_processed['Classification Day']/31)\n",
    "X_train_processed['DayOfWeek_sin'] = np.sin(2 * np.pi * X_train_processed['Classification DayOfWeek']/7)\n",
    "X_train_processed['DayOfWeek_cos'] = np.cos(2 * np.pi * X_train_processed['Classification DayOfWeek']/7)\n",
    "\n",
    "# Standardize year (continuous variable)\n",
    "min_year = X_train_processed['Classification Year'].min()\n",
    "X_train_processed['Years_Since_First'] = X_train_processed['Classification Year'] - min_year"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8607e8-2dae-42f9-b92c-fbda9b1eca12",
   "metadata": {},
   "source": [
    "#### Feature engineering for 'Recalling Firm Name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "819be4bd-53ee-4a52-855e-6b9c0682c368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract business structure from company name\n",
    "def extract_business_structure(firm_name):\n",
    "    \"\"\"Extract business structure from company name\"\"\"\n",
    "    if not isinstance(firm_name, str):\n",
    "        return 'Unknown'\n",
    "    \n",
    "    # Convert to uppercase for consistent pattern matching\n",
    "    name = firm_name.upper()\n",
    "    \n",
    "    # Define patterns to search for, in order of specificity\n",
    "    if any(x in name for x in [' LLC', ', LLC', ' L.L.C.', ', L.L.C.', 'LIMITED LIABILITY COMPANY']):\n",
    "        return 'LLC'\n",
    "    elif any(x in name for x in [' LP', ', LP', ' L.P.', ', L.P.', 'LIMITED PARTNERSHIP']):\n",
    "        return 'LP'\n",
    "    elif any(x in name for x in [' LLP', ', LLP', ' L.L.P.', ', L.L.P.', 'LIMITED LIABILITY PARTNERSHIP']):\n",
    "        return 'LLP'\n",
    "    elif any(x in name for x in [' CORP', ', CORP', ' CORPORATION', ', CORPORATION', ' CORP.', ', CORP.']):\n",
    "        return 'Corporation'\n",
    "    elif any(x in name for x in [' INC', ', INC', ' INC.', ', INC.', 'INCORPORATED']):\n",
    "        return 'Inc'\n",
    "    elif any(x in name for x in [' CO', ', CO', ' CO.', ', CO.', 'COMPANY']):\n",
    "        return 'Company'\n",
    "    elif any(x in name for x in ['ASSOCIATION', 'ASSOC.', 'ASSN']):\n",
    "        return 'Association'\n",
    "    elif any(x in name for x in ['PARTNERS', 'PARTNERSHIP']):\n",
    "        return 'Partnership'\n",
    "    elif any(x in name for x in ['FOUNDATION', 'NON-PROFIT', 'NONPROFIT', 'CHARITY']):\n",
    "        return 'Non-Profit'\n",
    "    elif any(x in name for x in [' PLC', ', PLC', 'PUBLIC LIMITED COMPANY']):\n",
    "        return 'PLC'\n",
    "    elif any(x in name for x in [' AG', ', AG', 'AKTIENGESELLSCHAFT']):\n",
    "        return 'AG'\n",
    "    elif any(x in name for x in [' SA', ', SA', 'SOCIEDAD ANONIMA']):\n",
    "        return 'SA'\n",
    "    elif any(x in name for x in [' GmbH', ', GmbH']):\n",
    "        return 'GmbH'\n",
    "    elif any(x in name for x in [' LTD', ', LTD', ' LIMITED', ', LIMITED']):\n",
    "        return 'Ltd'\n",
    "    else:\n",
    "        # If no business structure indicators found, sole proprietorship or classify as unknown\n",
    "        return 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1d1735e8-bb7f-4a27-a8be-e4a2f6ec2b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column with business structure\n",
    "X_train_processed['Business_Structure'] = X_train_processed['Recalling Firm Name'].apply(extract_business_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a5233e-3613-4402-a0f8-7852a5c6d060",
   "metadata": {},
   "source": [
    "#### Feature engineering for 'Recalling Firm Country'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e05b3a9-093e-4539-82be-ba77bf3c3289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary US indicator\n",
    "X_train_processed['Is_US'] = (X_train_processed['Recalling Firm Country'] == 'United States').astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9d01d7-6ee7-48d4-9269-8a3d12dcc71b",
   "metadata": {},
   "source": [
    "#### Create encoders during preprocessing of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bc09b53-6e41-49a7-9635-12badea7b5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary\n",
    "categorical_encoders = {}\n",
    "categorical_features = ['Product Classification', 'Product Type', 'Status', 'Business_Structure']\n",
    "\n",
    "# Create and fit encoders on training data\n",
    "for feature in categorical_features:\n",
    "    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore', drop='first')\n",
    "    encoder.fit(X_train_processed[[feature]])\n",
    "    categorical_encoders[feature] = encoder\n",
    "    \n",
    "    # Transform the training data\n",
    "    feature_array = encoder.transform(X_train_processed[[feature]])\n",
    "    feature_names = [f\"{feature.replace(' ', '')}_{cat}\" for cat in encoder.categories_[0][1:]]\n",
    "    \n",
    "    # Create DataFrame with proper column names and index\n",
    "    encoded_df = pd.DataFrame(\n",
    "        feature_array, \n",
    "        columns=feature_names,\n",
    "        index=X_train_processed.index\n",
    "    )\n",
    "    \n",
    "    # Concatenate with the processed dataframe\n",
    "    X_train_processed = pd.concat([X_train_processed, encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2600913-a16c-4d29-90cc-f8973ac1ffd9",
   "metadata": {},
   "source": [
    "#### Clean and Normalize Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c07ec7b4-ed27-4f89-acaf-db4e9acb7284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define text cleaning function\n",
    "def text_cleaner(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    # Convert to lowercase \n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove control characters and encoding artifacts\n",
    "    text = re.sub(r'\\*x[0-9a-f]{4}\\*', ' ', text)\n",
    "    text = re.sub(r'[\\x00-\\x1F\\x7F-\\x9F]', ' ', text)\n",
    "    \n",
    "    # Remove special characters but preserve important regulatory codes\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "    # Remove numbers but keep important regulatory codes intact\n",
    "    text = re.sub(r'\\b\\d+\\b(?!\\s*cfr|\\s*usc|\\s*fda)', ' ', text)\n",
    "    \n",
    "    # Remove extra whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Normalize common terms\n",
    "    text = re.sub(r'\\b(upc|sku|item|code|number)s?\\b', 'product_code', text)\n",
    "    text = re.sub(r'\\b(possible|potential|may|might|could)\\b', 'potential', text)\n",
    "    \n",
    "    # Group measurement units\n",
    "    text = re.sub(r'\\b\\d+\\s*(oz|ml|mg|g)\\b', 'quantity_measure', text)\n",
    "    \n",
    "    # Remove FDA-specific stopwords in addition to standard ones\n",
    "    stop_words = set(stopwords.words('english') + \n",
    "                    stopwords.words('spanish') + \n",
    "                    stopwords.words('french') + \n",
    "                    stopwords.words('german'))\n",
    "    \n",
    "    fda_stopwords = {'recalled', 'recalling', 'firm', 'product', 'products', 'recall', \n",
    "                     'various', 'due', 'manufactured'}\n",
    "    all_stopwords = stop_words.union(fda_stopwords)\n",
    "    \n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [w for w in tokens if w not in all_stopwords]\n",
    "    \n",
    "    return ' '.join(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28c95a1-ee27-4c78-8fc5-72320c4d2c84",
   "metadata": {},
   "source": [
    "#### Text Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5415055-d564-4022-aea3-e27a8a875d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the text data in training set\n",
    "X_train_text = X_train.copy()  # This should have the same index as X_train\n",
    "X_train_text['reason_cleaned'] = X_train_text['Reason for Recall'].apply(text_cleaner)\n",
    "X_train_text['description_cleaned'] = X_train_text['Product Description'].apply(text_cleaner)\n",
    "X_train_text['combined_text'] = X_train_text['reason_cleaned'] + ' ' + X_train_text['description_cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2677110a-fa78-4354-bee6-177c1bc12208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF features with n-grams (fit on training data)\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,\n",
    "    min_df=5,                   # Ignore terms that appear in fewer than 5 documents\n",
    "    max_df=0.7,                 # Ignore terms that appear in more than 70% of documents\n",
    "    ngram_range=(1, 2),         # Include unigrams and bigrams\n",
    "    use_idf=True,\n",
    "    sublinear_tf=True           # Apply sublinear tf scaling (1 + log(tf))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "64b6ab57-9ec0-413f-8bdd-a2ee729611f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform on training data\n",
    "tfidf_matrix_train = tfidf_vectorizer.fit_transform(X_train_text['combined_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff0785c0-423a-468d-9da9-308a9c7264d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of TF-IDF features created: 5000\n"
     ]
    }
   ],
   "source": [
    "# Get the feature names for interpretability\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print(f\"Number of TF-IDF features created: {len(feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4474c96-c0c1-4a7b-b601-7f5eb7e3c5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply dimensionality reduction (fit on training data)\n",
    "n_components = min(300, tfidf_matrix_train.shape[1] - 1)\n",
    "svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "tfidf_svd_train = svd.fit_transform(tfidf_matrix_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "205cc199-47b0-45ed-b0c2-a08fc6f2e6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained variance from 300 SVD components: 0.48\n"
     ]
    }
   ],
   "source": [
    "# Calculate explained variance\n",
    "explained_var = svd.explained_variance_ratio_.sum()\n",
    "print(f\"Explained variance from {n_components} SVD components: {explained_var:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4603baf1-8440-41e8-9be9-17175824b8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with the reduced features with X_train index\n",
    "train_text_features_df = pd.DataFrame(\n",
    "    tfidf_svd_train, \n",
    "    columns=[f'text_svd_{i}' for i in range(n_components)],\n",
    "    index=X_train.index  # X_train index\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75628cb7-3072-4e2a-a942-2f2b587d19ea",
   "metadata": {},
   "source": [
    "# Create pre-processed TRAINING dataset for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3608f66-2ccf-4036-afd3-8a082a768413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL CLEANUP - Drop all original columns now that feature engineering is complete\n",
    "original_cols_to_drop = [\n",
    "    # Original columns that have been transformed\n",
    "    \"Center\", \"Center Classification Date\",\n",
    "    \"Recalling Firm Name\", \"Distribution Pattern\",\n",
    "    \"Recalling Firm City\", \"Recalling Firm State\", \"Recalling Firm Country\",\n",
    "    \"Product Type\", \"Status\", \"Business_Structure\",\n",
    "    # Text columns that have been processed\n",
    "    \"Reason for Recall\", \"Product Description\",\n",
    "    # Original categorical column now fully dummy encoded\n",
    "    \"Product Classification\",\n",
    "    # Intermediate columns\n",
    "    \"Classification Year\", \"Classification Month\", \"Classification Day\", \"Classification DayOfWeek\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d60674c-af04-4555-bbd3-09025e22722b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to only include existing columns\n",
    "cols_to_drop = [col for col in original_cols_to_drop if col in X_train_processed.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "da025885-889b-4aca-8a83-88108c7ab62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "X_train_baseline = X_train_processed.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b4da03a-11df-4efe-a204-2e96c789162d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (76065, 329)\n"
     ]
    }
   ],
   "source": [
    "# Add the text features to the processed dataframe - use index-based joining\n",
    "X_train_final = pd.concat([X_train_baseline, train_text_features_df], axis=1)\n",
    "print(f\"Training dataset shape: {X_train_final.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd63d05-978f-4618-9d2c-0302b02b0a13",
   "metadata": {},
   "source": [
    "#### Process the TEST set using the same transformations learned from TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69c163a9-da48-43d6-b588-1378afddcc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the test data using the same steps and parameters learned from training\n",
    "X_test_processed = X_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75fb3b19-49b0-4eea-969f-71119c15b046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to datetime\n",
    "X_test_processed['Center Classification Date'] = pd.to_datetime(X_test_processed['Center Classification Date'], errors='coerce')\n",
    "\n",
    "# Extract temporal features\n",
    "X_test_processed['Classification Year'] = X_test_processed['Center Classification Date'].dt.year\n",
    "X_test_processed['Classification Month'] = X_test_processed['Center Classification Date'].dt.month\n",
    "X_test_processed['Classification Day'] = X_test_processed['Center Classification Date'].dt.day\n",
    "X_test_processed['Classification DayOfWeek'] = X_test_processed['Center Classification Date'].dt.dayofweek\n",
    "\n",
    "# Add cyclical encoding\n",
    "X_test_processed['Month_sin'] = np.sin(2 * np.pi * X_test_processed['Classification Month']/12)\n",
    "X_test_processed['Month_cos'] = np.cos(2 * np.pi * X_test_processed['Classification Month']/12)\n",
    "X_test_processed['Day_sin'] = np.sin(2 * np.pi * X_test_processed['Classification Day']/31)\n",
    "X_test_processed['Day_cos'] = np.cos(2 * np.pi * X_test_processed['Classification Day']/31)\n",
    "X_test_processed['DayOfWeek_sin'] = np.sin(2 * np.pi * X_test_processed['Classification DayOfWeek']/7)\n",
    "X_test_processed['DayOfWeek_cos'] = np.cos(2 * np.pi * X_test_processed['Classification DayOfWeek']/7)\n",
    "\n",
    "# Standardize year (continuous variable)\n",
    "# Use the same reference year from training\n",
    "X_test_processed['Years_Since_First'] = X_test_processed['Classification Year'] - min_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef0bf542-f82a-45b7-a7eb-83cc2b32d645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract business structure for test set\n",
    "X_test_processed['Business_Structure'] = X_test_processed['Recalling Firm Name'].apply(extract_business_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "540364aa-e67f-47d6-84fd-af45998a4ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country indicator for test set\n",
    "X_test_processed['Is_US'] = (X_test_processed['Recalling Firm Country'] == 'United States').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "800c96db-1041-4b55-80f7-aae89eab113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use encoders previously fit on training data\n",
    "for feature in categorical_features:\n",
    "    if feature in X_test_processed.columns:\n",
    "        # Transform using the encoder fit on training data\n",
    "        encoder = categorical_encoders[feature]\n",
    "        feature_array = encoder.transform(X_test_processed[[feature]])\n",
    "        \n",
    "        # Get the same feature names used in training\n",
    "        feature_names = [f\"{feature.replace(' ', '')}_{cat}\" for cat in encoder.categories_[0][1:]]\n",
    "        \n",
    "        # Create DataFrame with proper column names and index\n",
    "        encoded_df = pd.DataFrame(\n",
    "            feature_array, \n",
    "            columns=feature_names,\n",
    "            index=X_test_processed.index\n",
    "        )\n",
    "        \n",
    "        # Concatenate with the processed dataframe\n",
    "        X_test_processed = pd.concat([X_test_processed, encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "08a956fc-29f6-4cfa-a09b-4ef86731d0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text features - process using the same vectorizer and SVD fit on training\n",
    "X_test_text = X_test.copy()\n",
    "X_test_text['reason_cleaned'] = X_test_text['Reason for Recall'].apply(text_cleaner)\n",
    "X_test_text['description_cleaned'] = X_test_text['Product Description'].apply(text_cleaner)\n",
    "X_test_text['combined_text'] = X_test_text['reason_cleaned'] + ' ' + X_test_text['description_cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "122242b5-fb06-4ba3-a9da-351952054d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform using the vectorizer fit on training data\n",
    "tfidf_matrix_test = tfidf_vectorizer.transform(X_test_text['combined_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bbf228a5-5405-4511-9168-a75aba612825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test_text shape: (19017, 15), tfidf_matrix_test shape: (19017, 5000)\n"
     ]
    }
   ],
   "source": [
    "# Apply the SVD transformation fit on training data\n",
    "tfidf_svd_test = svd.transform(tfidf_matrix_test)\n",
    "print(f\"X_test_text shape: {X_test_text.shape}, tfidf_matrix_test shape: {tfidf_matrix_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8df49e09-ff56-4bd0-a52e-9974e51a1e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame with the reduced features with X_test index\n",
    "test_text_features_df = pd.DataFrame(\n",
    "    tfidf_svd_test, \n",
    "    columns=[f'text_svd_{i}' for i in range(n_components)],\n",
    "    index=X_test.index  # X_test index\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d60f8e9-cf72-454f-b550-ac6daf178ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup original columns (same as test set)\n",
    "X_test_baseline = X_test_processed.drop(columns=cols_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aff4f531-10a4-43f5-b3ca-0cf302f8cd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine structured and text features with index-based joining\n",
    "X_test_final = pd.concat([X_test_baseline, test_text_features_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d6c4f5e-5062-41c2-9f18-021dd91e1ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that test has same columns as train\n",
    "missing_cols = set(X_train_final.columns) - set(X_test_final.columns)\n",
    "for col in missing_cols:\n",
    "    X_test_final[col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a25f44b-8c6c-4923-b271-60780af35df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset shape: (19017, 329)\n"
     ]
    }
   ],
   "source": [
    "# Check columns are in the same order\n",
    "X_test_final = X_test_final[X_train_final.columns]\n",
    "print(f\"Test dataset shape: {X_test_final.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f4a4daf8-1ec0-407f-926a-6ee95ccadad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and test columns match\n"
     ]
    }
   ],
   "source": [
    "# Check both datasets have the same columns in the same order\n",
    "assert all(X_train_final.columns == X_test_final.columns), \"Column mismatch between train and test\"\n",
    "print(\"Training and test columns match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9356df82-1b50-4db2-aea0-0bb9c1f919cb",
   "metadata": {},
   "source": [
    "# Save the processed datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd924ad-0c76-4690-9a11-4ba759d3212f",
   "metadata": {},
   "source": [
    "Create BASELINE datasets (without text features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c919896-fc4c-496f-9c51-f0fb7668f032",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the target variable to the baseline training dataset\n",
    "train_baseline = X_train_baseline.copy()\n",
    "train_baseline['Event Classification'] = y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9c0a74c7-2572-4b92-a389-7a76aa7151dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Training set shape: (76065, 30)\n",
      "Baseline Test set shape: (19017, 30)\n",
      "\n",
      " Baseline Column names:\n",
      "['Month_sin', 'Month_cos', 'Day_sin', 'Day_cos', 'DayOfWeek_sin', 'DayOfWeek_cos', 'Years_Since_First', 'Is_US', 'ProductClassification_Class II', 'ProductClassification_Class III', 'ProductType_Devices', 'ProductType_Drugs', 'ProductType_Food/Cosmetics', 'ProductType_Tobacco', 'ProductType_Veterinary', 'Status_Ongoing', 'Status_Terminated', 'Business_Structure_Association', 'Business_Structure_Company', 'Business_Structure_Corporation', 'Business_Structure_Inc', 'Business_Structure_LLC', 'Business_Structure_LLP', 'Business_Structure_LP', 'Business_Structure_Ltd', 'Business_Structure_Non-Profit', 'Business_Structure_Other', 'Business_Structure_PLC', 'Business_Structure_SA', 'Event Classification']\n"
     ]
    }
   ],
   "source": [
    "# Add the target variable to the baseline test dataset\n",
    "test_baseline = X_test_baseline.copy()\n",
    "test_baseline['Event Classification'] = y_test.values\n",
    "\n",
    "print(f\"Baseline Training set shape: {train_baseline.shape}\")\n",
    "print(f\"Baseline Test set shape: {test_baseline.shape}\")\n",
    "print(\"\\n Baseline Column names:\")\n",
    "print(test_baseline.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b514e9-1081-4d0a-823c-27457ea6e1d4",
   "metadata": {},
   "source": [
    "Create HYBRID datasets (with text features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "28ab865c-94a4-4096-9444-0a8e86e73d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the target variable to the hybrid training dataset\n",
    "train_hybrid = X_train_final.copy()\n",
    "train_hybrid['Event Classification'] = y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf9939ce-92a7-4b96-bbee-e9f788d7acfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Training set shape: (76065, 330)\n",
      "Hybrid Test set shape: (19017, 330)\n",
      "\n",
      " Hybrid Column names:\n",
      "['Month_sin', 'Month_cos', 'Day_sin', 'Day_cos', 'DayOfWeek_sin', 'DayOfWeek_cos', 'Years_Since_First', 'Is_US', 'ProductClassification_Class II', 'ProductClassification_Class III', 'ProductType_Devices', 'ProductType_Drugs', 'ProductType_Food/Cosmetics', 'ProductType_Tobacco', 'ProductType_Veterinary', 'Status_Ongoing', 'Status_Terminated', 'Business_Structure_Association', 'Business_Structure_Company', 'Business_Structure_Corporation', 'Business_Structure_Inc', 'Business_Structure_LLC', 'Business_Structure_LLP', 'Business_Structure_LP', 'Business_Structure_Ltd', 'Business_Structure_Non-Profit', 'Business_Structure_Other', 'Business_Structure_PLC', 'Business_Structure_SA', 'text_svd_0', 'text_svd_1', 'text_svd_2', 'text_svd_3', 'text_svd_4', 'text_svd_5', 'text_svd_6', 'text_svd_7', 'text_svd_8', 'text_svd_9', 'text_svd_10', 'text_svd_11', 'text_svd_12', 'text_svd_13', 'text_svd_14', 'text_svd_15', 'text_svd_16', 'text_svd_17', 'text_svd_18', 'text_svd_19', 'text_svd_20', 'text_svd_21', 'text_svd_22', 'text_svd_23', 'text_svd_24', 'text_svd_25', 'text_svd_26', 'text_svd_27', 'text_svd_28', 'text_svd_29', 'text_svd_30', 'text_svd_31', 'text_svd_32', 'text_svd_33', 'text_svd_34', 'text_svd_35', 'text_svd_36', 'text_svd_37', 'text_svd_38', 'text_svd_39', 'text_svd_40', 'text_svd_41', 'text_svd_42', 'text_svd_43', 'text_svd_44', 'text_svd_45', 'text_svd_46', 'text_svd_47', 'text_svd_48', 'text_svd_49', 'text_svd_50', 'text_svd_51', 'text_svd_52', 'text_svd_53', 'text_svd_54', 'text_svd_55', 'text_svd_56', 'text_svd_57', 'text_svd_58', 'text_svd_59', 'text_svd_60', 'text_svd_61', 'text_svd_62', 'text_svd_63', 'text_svd_64', 'text_svd_65', 'text_svd_66', 'text_svd_67', 'text_svd_68', 'text_svd_69', 'text_svd_70', 'text_svd_71', 'text_svd_72', 'text_svd_73', 'text_svd_74', 'text_svd_75', 'text_svd_76', 'text_svd_77', 'text_svd_78', 'text_svd_79', 'text_svd_80', 'text_svd_81', 'text_svd_82', 'text_svd_83', 'text_svd_84', 'text_svd_85', 'text_svd_86', 'text_svd_87', 'text_svd_88', 'text_svd_89', 'text_svd_90', 'text_svd_91', 'text_svd_92', 'text_svd_93', 'text_svd_94', 'text_svd_95', 'text_svd_96', 'text_svd_97', 'text_svd_98', 'text_svd_99', 'text_svd_100', 'text_svd_101', 'text_svd_102', 'text_svd_103', 'text_svd_104', 'text_svd_105', 'text_svd_106', 'text_svd_107', 'text_svd_108', 'text_svd_109', 'text_svd_110', 'text_svd_111', 'text_svd_112', 'text_svd_113', 'text_svd_114', 'text_svd_115', 'text_svd_116', 'text_svd_117', 'text_svd_118', 'text_svd_119', 'text_svd_120', 'text_svd_121', 'text_svd_122', 'text_svd_123', 'text_svd_124', 'text_svd_125', 'text_svd_126', 'text_svd_127', 'text_svd_128', 'text_svd_129', 'text_svd_130', 'text_svd_131', 'text_svd_132', 'text_svd_133', 'text_svd_134', 'text_svd_135', 'text_svd_136', 'text_svd_137', 'text_svd_138', 'text_svd_139', 'text_svd_140', 'text_svd_141', 'text_svd_142', 'text_svd_143', 'text_svd_144', 'text_svd_145', 'text_svd_146', 'text_svd_147', 'text_svd_148', 'text_svd_149', 'text_svd_150', 'text_svd_151', 'text_svd_152', 'text_svd_153', 'text_svd_154', 'text_svd_155', 'text_svd_156', 'text_svd_157', 'text_svd_158', 'text_svd_159', 'text_svd_160', 'text_svd_161', 'text_svd_162', 'text_svd_163', 'text_svd_164', 'text_svd_165', 'text_svd_166', 'text_svd_167', 'text_svd_168', 'text_svd_169', 'text_svd_170', 'text_svd_171', 'text_svd_172', 'text_svd_173', 'text_svd_174', 'text_svd_175', 'text_svd_176', 'text_svd_177', 'text_svd_178', 'text_svd_179', 'text_svd_180', 'text_svd_181', 'text_svd_182', 'text_svd_183', 'text_svd_184', 'text_svd_185', 'text_svd_186', 'text_svd_187', 'text_svd_188', 'text_svd_189', 'text_svd_190', 'text_svd_191', 'text_svd_192', 'text_svd_193', 'text_svd_194', 'text_svd_195', 'text_svd_196', 'text_svd_197', 'text_svd_198', 'text_svd_199', 'text_svd_200', 'text_svd_201', 'text_svd_202', 'text_svd_203', 'text_svd_204', 'text_svd_205', 'text_svd_206', 'text_svd_207', 'text_svd_208', 'text_svd_209', 'text_svd_210', 'text_svd_211', 'text_svd_212', 'text_svd_213', 'text_svd_214', 'text_svd_215', 'text_svd_216', 'text_svd_217', 'text_svd_218', 'text_svd_219', 'text_svd_220', 'text_svd_221', 'text_svd_222', 'text_svd_223', 'text_svd_224', 'text_svd_225', 'text_svd_226', 'text_svd_227', 'text_svd_228', 'text_svd_229', 'text_svd_230', 'text_svd_231', 'text_svd_232', 'text_svd_233', 'text_svd_234', 'text_svd_235', 'text_svd_236', 'text_svd_237', 'text_svd_238', 'text_svd_239', 'text_svd_240', 'text_svd_241', 'text_svd_242', 'text_svd_243', 'text_svd_244', 'text_svd_245', 'text_svd_246', 'text_svd_247', 'text_svd_248', 'text_svd_249', 'text_svd_250', 'text_svd_251', 'text_svd_252', 'text_svd_253', 'text_svd_254', 'text_svd_255', 'text_svd_256', 'text_svd_257', 'text_svd_258', 'text_svd_259', 'text_svd_260', 'text_svd_261', 'text_svd_262', 'text_svd_263', 'text_svd_264', 'text_svd_265', 'text_svd_266', 'text_svd_267', 'text_svd_268', 'text_svd_269', 'text_svd_270', 'text_svd_271', 'text_svd_272', 'text_svd_273', 'text_svd_274', 'text_svd_275', 'text_svd_276', 'text_svd_277', 'text_svd_278', 'text_svd_279', 'text_svd_280', 'text_svd_281', 'text_svd_282', 'text_svd_283', 'text_svd_284', 'text_svd_285', 'text_svd_286', 'text_svd_287', 'text_svd_288', 'text_svd_289', 'text_svd_290', 'text_svd_291', 'text_svd_292', 'text_svd_293', 'text_svd_294', 'text_svd_295', 'text_svd_296', 'text_svd_297', 'text_svd_298', 'text_svd_299', 'Event Classification']\n"
     ]
    }
   ],
   "source": [
    "# Add the target variable to the hybrid test dataset\n",
    "test_hybrid = X_test_final.copy()\n",
    "test_hybrid['Event Classification'] = y_test.values\n",
    "\n",
    "print(f\"Hybrid Training set shape: {train_hybrid.shape}\")\n",
    "print(f\"Hybrid Test set shape: {test_hybrid.shape}\")\n",
    "print(\"\\n Hybrid Column names:\")\n",
    "print(test_hybrid.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f23cfc1c-736b-42fe-8893-a3b908dc70f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline datasets\n",
    "train_baseline.to_csv('../data/train_baseline.csv', index=False)\n",
    "test_baseline.to_csv('../data/test_baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "946a6ea6-66e5-40da-9fa1-836d7f329412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid datasets\n",
    "train_hybrid.to_csv('../data/train_hybrid.csv', index=False)\n",
    "test_hybrid.to_csv('../data/test_hybrid.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
