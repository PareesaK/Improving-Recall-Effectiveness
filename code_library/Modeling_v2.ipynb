{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6cdfc50-7af3-40af-b21e-591835feca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling notebook enhanced by https://claude.ai/\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import (classification_report, accuracy_score, \n",
    "                            confusion_matrix, ConfusionMatrixDisplay, \n",
    "                            roc_curve, auc)\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from scipy.stats import randint, uniform\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b76d36e6-3194-4043-959b-9fddd40a804f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories for saving results\n",
    "os.makedirs('results', exist_ok=True)\n",
    "os.makedirs('results/baseline', exist_ok=True)\n",
    "os.makedirs('results/hybrid', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e41a664-59cd-42c9-a797-10f1fc2bff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eeedfb8-8224-495c-83a3-ce556d987ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_modeling_pipeline(dataset_type='baseline'):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"    MODELING PIPELINE FOR {dataset_type.upper()} DATASET\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Step 1: Exploratory Data Analysis (EDA)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"1. EXPLORATORY DATA ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Load the dataset\n",
    "    train_path = f'../data/train_{dataset_type}.csv'\n",
    "    test_path = f'../data/test_{dataset_type}.csv'\n",
    "    \n",
    "    try:\n",
    "        train_df = pd.read_csv(train_path)\n",
    "        test_df = pd.read_csv(test_path)\n",
    "        print(f\"Successfully loaded {dataset_type} datasets:\")\n",
    "        print(f\"Training shape: {train_df.shape}\")\n",
    "        print(f\"Testing shape: {test_df.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not find {dataset_type} dataset files. Please check file paths.\")\n",
    "        return\n",
    "    \n",
    "    # Basic dataset information\n",
    "    print(\"\\nTraining Dataset Overview:\")\n",
    "    print(f\"Columns: {train_df.columns.tolist()}\")\n",
    "    print(\"\\nData Types:\")\n",
    "    print(train_df.dtypes)\n",
    "    \n",
    "    # Target distribution\n",
    "    target_col = 'Event Classification'\n",
    "    target_dist = train_df[target_col].value_counts(normalize=True).reset_index()\n",
    "    target_dist.columns = ['Class', 'Proportion']\n",
    "    print(\"\\nTarget Distribution:\")\n",
    "    print(target_dist)\n",
    "    \n",
    "    # Visualize target distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x='Class', y='Proportion', data=target_dist)\n",
    "    plt.title(f'Target Distribution - {dataset_type.capitalize()} Dataset')\n",
    "    plt.ylabel('Proportion')\n",
    "    plt.savefig(f'results/{dataset_type}/target_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Step 2: Data Cleaning (Validation)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"2. DATA CLEANING VALIDATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing_values = train_df.isnull().sum()\n",
    "    print(\"\\nMissing values in training data:\")\n",
    "    print(missing_values[missing_values > 0] if missing_values.sum() > 0 else \"No missing values found\")\n",
    "    \n",
    "    # Step 3: Data Preparation\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"3. DATA PREPARATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Define features (X) and target (y)\n",
    "    X_train = train_df.drop(target_col, axis=1)\n",
    "    y_train = train_df[target_col]\n",
    "    X_test = test_df.drop(target_col, axis=1)\n",
    "    y_test = test_df[target_col]\n",
    "    \n",
    "    # Ensure test data has the same columns as train data\n",
    "    missing_cols = set(X_train.columns) - set(X_test.columns)\n",
    "    for col in missing_cols:\n",
    "        X_test[col] = 0\n",
    "    X_test = X_test[X_train.columns]\n",
    "    \n",
    "    # Encode target\n",
    "    le = LabelEncoder()\n",
    "    y_train_encoded = le.fit_transform(y_train)\n",
    "    y_test_encoded = le.transform(y_test)\n",
    "    \n",
    "    class_names = le.classes_\n",
    "    print(f\"Target Classes: {class_names}\")\n",
    "    \n",
    "    # Step 4: Train-Test Split Validation\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"4. TRAIN-TEST SPLIT VALIDATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # The split was done in preprocessing, but validate proportions\n",
    "    print(\"Training set size:\", X_train.shape[0])\n",
    "    print(\"Test set size:\", X_test.shape[0])\n",
    "    print(\"Test set proportion: {:.2f}%\".format(100 * X_test.shape[0] / (X_train.shape[0] + X_test.shape[0])))\n",
    "    \n",
    "    print(\"\\nTraining class distribution:\")\n",
    "    print(pd.Series(y_train_encoded).value_counts(normalize=True))\n",
    "    \n",
    "    print(\"\\nTest class distribution:\")\n",
    "    print(pd.Series(y_test_encoded).value_counts(normalize=True))\n",
    "    \n",
    "    # Step 5: Feature Engineering Validation\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"5. FEATURE ENGINEERING VALIDATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Display number of features\n",
    "    print(f\"Number of features: {X_train.shape[1]}\")\n",
    "    \n",
    "    # Display feature categories\n",
    "    temporal_features = [col for col in X_train.columns if any(x in col for x in ['Month', 'Day', 'Year', 'Week'])]\n",
    "    text_features = [col for col in X_train.columns if 'text_svd_' in col]\n",
    "    categorical_features = [col for col in X_train.columns if any(x in col for x in ['Classification', 'Type', 'Status', 'Structure'])]\n",
    "    \n",
    "    print(f\"Temporal features: {len(temporal_features)}\")\n",
    "    print(f\"Text-derived features: {len(text_features)}\")\n",
    "    print(f\"Categorical features: {len(categorical_features)}\")\n",
    "    \n",
    "    # Step 6: Pre-processed Dataset Validation\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"6. PRE-PROCESSED DATASET VALIDATION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Check for any remaining issues\n",
    "    print(\"Checking for infinity or NaN values...\")\n",
    "    inf_count = np.isinf(X_train.values).sum()\n",
    "    nan_count = np.isnan(X_train.values).sum()\n",
    "    \n",
    "    if inf_count > 0 or nan_count > 0:\n",
    "        print(f\"Warning: Found {inf_count} infinity values and {nan_count} NaN values\")\n",
    "        # Handle inf/nan values if necessary\n",
    "        X_train = X_train.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "        X_test = X_test.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "    else:\n",
    "        print(\"No infinity or NaN values found.\")\n",
    "    \n",
    "    # Step 7: Modeling\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"7. MODELING\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Initialize models to test\n",
    "    models = {\n",
    "        'Random Forest': RandomForestClassifier(random_state=42, class_weight='balanced'),\n",
    "        'XGBoost': XGBClassifier(eval_metric='mlogloss', random_state=42),\n",
    "        'CatBoost': CatBoostClassifier(verbose=0, random_state=42),\n",
    "        'MLPClassifier': MLPClassifier(hidden_layer_sizes=(100,), max_iter=300, random_state=42)\n",
    "    }\n",
    "    \n",
    "    # Step 8-13: Cross-validation with SMOTE and Feature Selection\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"8-13. CROSS-VALIDATION WITH SMOTE & FEATURE SELECTION\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Set up K-fold cross-validation\n",
    "    n_folds = 5\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Track results for each model\n",
    "    cv_results = {}\n",
    "    feature_importance_counts = pd.DataFrame(0, index=X_train.columns, columns=list(models.keys()))\n",
    "    \n",
    "    # For each model\n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nEvaluating {model_name}...\")\n",
    "        \n",
    "        fold_accuracies = []\n",
    "        selected_features_per_fold = []\n",
    "        \n",
    "        # For each fold\n",
    "        for fold, (train_idx, valid_idx) in enumerate(skf.split(X_train, y_train_encoded), 1):\n",
    "            print(f\"  Fold {fold}/{n_folds}\")\n",
    "            \n",
    "            # Split data for this fold\n",
    "            X_fold_train, X_fold_valid = X_train.iloc[train_idx], X_train.iloc[valid_idx]\n",
    "            y_fold_train, y_fold_valid = y_train_encoded[train_idx], y_train_encoded[valid_idx]\n",
    "            \n",
    "            # Step 9: Apply SMOTE on training portion\n",
    "            sm = SMOTE(random_state=42)\n",
    "            X_fold_train_resampled, y_fold_train_resampled = sm.fit_resample(X_fold_train, y_fold_train)\n",
    "            print(f\"    Applied SMOTE: {X_fold_train.shape} → {X_fold_train_resampled.shape}\")\n",
    "            \n",
    "            # Step 10: Feature Selection on training portion of fold\n",
    "            if model_name in ['Random Forest', 'XGBoost', 'CatBoost']:\n",
    "                # Initialize selector\n",
    "                if model_name == 'Random Forest':\n",
    "                    selector_model = RandomForestClassifier(random_state=42)\n",
    "                elif model_name == 'XGBoost':\n",
    "                    selector_model = XGBClassifier(eval_metric='mlogloss', random_state=42)\n",
    "                else:  # CatBoost\n",
    "                    selector_model = CatBoostClassifier(verbose=0, random_state=42)\n",
    "                \n",
    "                # Fit selector model\n",
    "                selector_model.fit(X_fold_train_resampled, y_fold_train_resampled)\n",
    "                \n",
    "                # Select features based on importance\n",
    "                selector = SelectFromModel(selector_model, threshold='mean', prefit=True)\n",
    "                X_fold_train_selected = selector.transform(X_fold_train_resampled)\n",
    "                X_fold_valid_selected = selector.transform(X_fold_valid)\n",
    "                \n",
    "                # Get selected feature names\n",
    "                selected_mask = selector.get_support()\n",
    "                selected_features = X_train.columns[selected_mask].tolist()\n",
    "                selected_features_per_fold.append(selected_features)\n",
    "                \n",
    "                # Update feature importance count\n",
    "                for feature in selected_features:\n",
    "                    feature_importance_counts.loc[feature, model_name] += 1\n",
    "                \n",
    "                print(f\"    Selected {len(selected_features)} features\")\n",
    "            else:\n",
    "                # For MLP, use all features\n",
    "                X_fold_train_selected = X_fold_train_resampled\n",
    "                X_fold_valid_selected = X_fold_valid\n",
    "                selected_features_per_fold.append(X_train.columns.tolist())\n",
    "            \n",
    "            # Step 11: Train model with selected features\n",
    "            current_model = models[model_name]\n",
    "            current_model.fit(X_fold_train_selected, y_fold_train_resampled)\n",
    "            \n",
    "            # Step 13: Evaluate on validation portion\n",
    "            y_fold_pred = current_model.predict(X_fold_valid_selected)\n",
    "            fold_accuracy = accuracy_score(y_fold_valid, y_fold_pred)\n",
    "            fold_accuracies.append(fold_accuracy)\n",
    "            \n",
    "            print(f\"    Fold accuracy: {fold_accuracy:.4f}\")\n",
    "        \n",
    "        # Calculate average performance\n",
    "        mean_accuracy = np.mean(fold_accuracies)\n",
    "        std_accuracy = np.std(fold_accuracies)\n",
    "        \n",
    "        print(f\"  {model_name} - {n_folds}-Fold CV Results:\")\n",
    "        print(f\"  Mean Accuracy: {mean_accuracy:.4f} (±{std_accuracy:.4f})\")\n",
    "        \n",
    "        # Count most frequently selected features\n",
    "        if model_name in ['Random Forest', 'XGBoost', 'CatBoost']:\n",
    "            all_selected_features = [feature for fold_features in selected_features_per_fold for feature in fold_features]\n",
    "            feature_counts = pd.Series(all_selected_features).value_counts()\n",
    "            most_common_features = feature_counts[feature_counts >= 3].index.tolist()\n",
    "            print(f\"  Features selected in at least 3 folds: {len(most_common_features)}\")\n",
    "        \n",
    "        # Save model results\n",
    "        cv_results[model_name] = {\n",
    "            'fold_accuracies': fold_accuracies,\n",
    "            'mean_accuracy': mean_accuracy,\n",
    "            'std_accuracy': std_accuracy,\n",
    "            'selected_features_per_fold': selected_features_per_fold\n",
    "        }\n",
    "    \n",
    "    # Step 14: Select best model and feature set\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"14. SELECT BEST MODEL AND FEATURE SET\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    cv_summary = pd.DataFrame({\n",
    "        'Model': list(cv_results.keys()),\n",
    "        'Mean Accuracy': [results['mean_accuracy'] for results in cv_results.values()],\n",
    "        'Std Dev': [results['std_accuracy'] for results in cv_results.values()]\n",
    "    }).sort_values('Mean Accuracy', ascending=False)\n",
    "    \n",
    "    print(\"Cross-validation results summary:\")\n",
    "    print(cv_summary)\n",
    "    \n",
    "    # Select best model\n",
    "    best_model_name = cv_summary.iloc[0]['Model']\n",
    "    print(f\"\\nBest model: {best_model_name}\")\n",
    "    \n",
    "    # For the best model, identify most frequently selected features\n",
    "    if best_model_name in ['Random Forest', 'XGBoost', 'CatBoost']:\n",
    "        feature_importance = feature_importance_counts[best_model_name].sort_values(ascending=False)\n",
    "        \n",
    "        # Features selected in at least 3 folds\n",
    "        best_features = feature_importance[feature_importance >= 3].index.tolist()\n",
    "        \n",
    "        # If less than 20 features, take top 20\n",
    "        if len(best_features) < 20:\n",
    "            best_features = feature_importance.nlargest(20).index.tolist()\n",
    "        \n",
    "        print(f\"Selected {len(best_features)} features for final model\")\n",
    "    else:\n",
    "        # For MLP, use all features\n",
    "        best_features = X_train.columns.tolist()\n",
    "        print(f\"Using all {len(best_features)} features for final model (MLP)\")\n",
    "    \n",
    "    # Step 12: Hyperparameter Tuning for best model\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"12. HYPERPARAMETER TUNING\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Select subset of data with best features\n",
    "    X_train_best = X_train[best_features]\n",
    "    X_test_best = X_test[best_features]\n",
    "    \n",
    "    # Define hyperparameter grid for the best model\n",
    "    if best_model_name == \"Random Forest\":\n",
    "        model = RandomForestClassifier(random_state=42)\n",
    "        param_grid = {\n",
    "            'n_estimators': randint(100, 300),\n",
    "            'max_depth': randint(5, 30),\n",
    "            'min_samples_split': randint(2, 10),\n",
    "            'min_samples_leaf': randint(1, 5),\n",
    "            'max_features': ['sqrt', 'log2', None]\n",
    "        }\n",
    "    \n",
    "    elif best_model_name == \"XGBoost\":\n",
    "        model = XGBClassifier(eval_metric='mlogloss', random_state=42)\n",
    "        param_grid = {\n",
    "            'n_estimators': randint(100, 300),\n",
    "            'max_depth': randint(3, 15),\n",
    "            'learning_rate': uniform(0.01, 0.3),\n",
    "            'subsample': uniform(0.5, 0.5),\n",
    "            'colsample_bytree': uniform(0.5, 0.5)\n",
    "        }\n",
    "    \n",
    "    elif best_model_name == \"CatBoost\":\n",
    "        model = CatBoostClassifier(verbose=0, random_state=42)\n",
    "        param_grid = {\n",
    "            'iterations': randint(100, 500),\n",
    "            'depth': randint(4, 10),\n",
    "            'learning_rate': uniform(0.01, 0.3),\n",
    "            'l2_leaf_reg': uniform(1, 10),\n",
    "            'border_count': randint(32, 255)\n",
    "        }\n",
    "    \n",
    "    else:  # MLPClassifier\n",
    "        model = MLPClassifier(random_state=42)\n",
    "        param_grid = {\n",
    "            'hidden_layer_sizes': [(50,), (100,), (100, 50), (50, 50, 50)],\n",
    "            'activation': ['relu', 'tanh'],\n",
    "            'solver': ['adam'],\n",
    "            'alpha': [1e-5, 1e-4, 1e-3],\n",
    "            'learning_rate': ['constant', 'adaptive'],\n",
    "            'max_iter': [300, 500]\n",
    "        }\n",
    "    \n",
    "    # Run RandomizedSearchCV\n",
    "    print(f\"Tuning hyperparameters for {best_model_name}...\")\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_grid,\n",
    "        n_iter=20,\n",
    "        cv=5,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    start_time = time.time()\n",
    "    random_search.fit(X_train_best, y_train_encoded)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Hyperparameter tuning completed in {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Best parameters: {random_search.best_params_}\")\n",
    "    print(f\"Best CV accuracy: {random_search.best_score_:.4f}\")\n",
    "    \n",
    "    # Step 15: Train best model on full training set with best parameters\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"15. TRAIN FINAL MODEL\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Create final model with best parameters\n",
    "    if best_model_name == \"Random Forest\":\n",
    "        final_model = RandomForestClassifier(random_state=42, **random_search.best_params_)\n",
    "    elif best_model_name == \"XGBoost\":\n",
    "        final_model = XGBClassifier(eval_metric='mlogloss', random_state=42, **random_search.best_params_)\n",
    "    elif best_model_name == \"CatBoost\":\n",
    "        final_model = CatBoostClassifier(verbose=0, random_state=42, **random_search.best_params_)\n",
    "    else:  # MLPClassifier\n",
    "        final_model = MLPClassifier(random_state=42, **random_search.best_params_)\n",
    "    \n",
    "    # Apply SMOTE on full training set\n",
    "    print(\"Applying SMOTE on full training set...\")\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_train_best_resampled, y_train_encoded_resampled = sm.fit_resample(X_train_best, y_train_encoded)\n",
    "    print(f\"Training data shape after SMOTE: {X_train_best_resampled.shape}\")\n",
    "    \n",
    "    # Train final model\n",
    "    print(f\"Training final {best_model_name} model...\")\n",
    "    final_model.fit(X_train_best_resampled, y_train_encoded_resampled)\n",
    "    print(\"Final model training complete.\")\n",
    "    \n",
    "    # Step 16: Model Evaluation on Test Set\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"16. MODEL EVALUATION ON TEST SET\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Predict on test set\n",
    "    y_pred = final_model.predict(X_test_best)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test_encoded, y_pred)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    report = classification_report(y_test_encoded, y_pred, target_names=class_names)\n",
    "    print(report)\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_test_encoded, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    disp.plot(cmap='Blues', values_format='d')\n",
    "    plt.title(f'Confusion Matrix - {best_model_name}')\n",
    "    plt.savefig(f'results/{dataset_type}/{best_model_name}_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Normalized confusion matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm_normalized, display_labels=class_names)\n",
    "    disp.plot(cmap='Blues', values_format='.2f')\n",
    "    plt.title(f'Normalized Confusion Matrix - {best_model_name}')\n",
    "    plt.savefig(f'results/{dataset_type}/{best_model_name}_normalized_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Feature importance for tree-based models\n",
    "    if best_model_name in [\"Random Forest\", \"XGBoost\", \"CatBoost\"]:\n",
    "        # Get feature importances\n",
    "        if best_model_name == \"Random Forest\":\n",
    "            importances = final_model.feature_importances_\n",
    "        elif best_model_name == \"XGBoost\":\n",
    "            importances = final_model.feature_importances_\n",
    "        else:  # CatBoost\n",
    "            importances = final_model.get_feature_importance()\n",
    "        \n",
    "        # Create DataFrame for plotting\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': best_features,\n",
    "            'Importance': importances\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        # Plot top 15 features\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.barplot(x='Importance', y='Feature', data=importance_df.head(15))\n",
    "        plt.title(f'Top 15 Feature Importances - {best_model_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'results/{dataset_type}/{best_model_name}_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # ROC curves for multiclass\n",
    "    if hasattr(final_model, \"predict_proba\"):\n",
    "        # Binarize the output\n",
    "        y_test_bin = label_binarize(y_test_encoded, classes=range(len(class_names)))\n",
    "        y_score = final_model.predict_proba(X_test_best)\n",
    "        \n",
    "        # Compute ROC curve and ROC area for each class\n",
    "        fpr = dict()\n",
    "        tpr = dict()\n",
    "        roc_auc = dict()\n",
    "        \n",
    "        for i in range(len(class_names)):\n",
    "            fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
    "            roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        \n",
    "        # Plot all ROC curves\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        for i in range(len(class_names)):\n",
    "            plt.plot(fpr[i], tpr[i], lw=2,\n",
    "                    label=f'{class_names[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "        \n",
    "        plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "        plt.xlim([0.0, 1.0])\n",
    "        plt.ylim([0.0, 1.05])\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title(f'ROC Curves - {best_model_name}')\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.savefig(f'results/{dataset_type}/{best_model_name}_roc_curves.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "    \n",
    "    # Save final results\n",
    "    results = {\n",
    "        'dataset_type': dataset_type,\n",
    "        'best_model': best_model_name,\n",
    "        'best_features': best_features,\n",
    "        'best_params': random_search.best_params_,\n",
    "        'cv_accuracy': random_search.best_score_,\n",
    "        'test_accuracy': accuracy,\n",
    "        'classification_report': classification_report(y_test_encoded, y_pred, target_names=class_names, output_dict=True)\n",
    "    }\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4b3938-7641-4f62-a19f-ad28926df097",
   "metadata": {},
   "source": [
    "Run the complete pipeline for both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5193912-f0c0-48ed-ae01-61f5c4ba7572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************************************\n",
      "RUNNING MODELING PIPELINE FOR BASELINE DATASET\n",
      "********************************************************************************\n",
      "\n",
      "================================================================================\n",
      "    MODELING PIPELINE FOR BASELINE DATASET\n",
      "================================================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "1. EXPLORATORY DATA ANALYSIS\n",
      "==================================================\n",
      "Successfully loaded baseline datasets:\n",
      "Training shape: (76065, 30)\n",
      "Testing shape: (19017, 30)\n",
      "\n",
      "Training Dataset Overview:\n",
      "Columns: ['Month_sin', 'Month_cos', 'Day_sin', 'Day_cos', 'DayOfWeek_sin', 'DayOfWeek_cos', 'Years_Since_First', 'Is_US', 'ProductClassification_Class II', 'ProductClassification_Class III', 'ProductType_Devices', 'ProductType_Drugs', 'ProductType_Food/Cosmetics', 'ProductType_Tobacco', 'ProductType_Veterinary', 'Status_Ongoing', 'Status_Terminated', 'Business_Structure_Association', 'Business_Structure_Company', 'Business_Structure_Corporation', 'Business_Structure_Inc', 'Business_Structure_LLC', 'Business_Structure_LLP', 'Business_Structure_LP', 'Business_Structure_Ltd', 'Business_Structure_Non-Profit', 'Business_Structure_Other', 'Business_Structure_PLC', 'Business_Structure_SA', 'Event Classification']\n",
      "\n",
      "Data Types:\n",
      "Month_sin                          float64\n",
      "Month_cos                          float64\n",
      "Day_sin                            float64\n",
      "Day_cos                            float64\n",
      "DayOfWeek_sin                      float64\n",
      "DayOfWeek_cos                      float64\n",
      "Years_Since_First                    int64\n",
      "Is_US                                int64\n",
      "ProductClassification_Class II     float64\n",
      "ProductClassification_Class III    float64\n",
      "ProductType_Devices                float64\n",
      "ProductType_Drugs                  float64\n",
      "ProductType_Food/Cosmetics         float64\n",
      "ProductType_Tobacco                float64\n",
      "ProductType_Veterinary             float64\n",
      "Status_Ongoing                     float64\n",
      "Status_Terminated                  float64\n",
      "Business_Structure_Association     float64\n",
      "Business_Structure_Company         float64\n",
      "Business_Structure_Corporation     float64\n",
      "Business_Structure_Inc             float64\n",
      "Business_Structure_LLC             float64\n",
      "Business_Structure_LLP             float64\n",
      "Business_Structure_LP              float64\n",
      "Business_Structure_Ltd             float64\n",
      "Business_Structure_Non-Profit      float64\n",
      "Business_Structure_Other           float64\n",
      "Business_Structure_PLC             float64\n",
      "Business_Structure_SA              float64\n",
      "Event Classification                object\n",
      "dtype: object\n",
      "\n",
      "Target Distribution:\n",
      "       Class  Proportion\n",
      "0   Class II    0.708065\n",
      "1    Class I    0.211516\n",
      "2  Class III    0.080418\n",
      "\n",
      "==================================================\n",
      "2. DATA CLEANING VALIDATION\n",
      "==================================================\n",
      "\n",
      "Missing values in training data:\n",
      "No missing values found\n",
      "\n",
      "==================================================\n",
      "3. DATA PREPARATION\n",
      "==================================================\n",
      "Target Classes: ['Class I' 'Class II' 'Class III']\n",
      "\n",
      "==================================================\n",
      "4. TRAIN-TEST SPLIT VALIDATION\n",
      "==================================================\n",
      "Training set size: 76065\n",
      "Test set size: 19017\n",
      "Test set proportion: 20.00%\n",
      "\n",
      "Training class distribution:\n",
      "1    0.708065\n",
      "0    0.211516\n",
      "2    0.080418\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test class distribution:\n",
      "1    0.708103\n",
      "0    0.211495\n",
      "2    0.080402\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "==================================================\n",
      "5. FEATURE ENGINEERING VALIDATION\n",
      "==================================================\n",
      "Number of features: 29\n",
      "Temporal features: 7\n",
      "Text-derived features: 0\n",
      "Categorical features: 21\n",
      "\n",
      "==================================================\n",
      "6. PRE-PROCESSED DATASET VALIDATION\n",
      "==================================================\n",
      "Checking for infinity or NaN values...\n",
      "No infinity or NaN values found.\n",
      "\n",
      "==================================================\n",
      "7. MODELING\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "8-13. CROSS-VALIDATION WITH SMOTE & FEATURE SELECTION\n",
      "==================================================\n",
      "\n",
      "Evaluating Random Forest...\n",
      "  Fold 1/5\n",
      "    Applied SMOTE: (60852, 29) → (129261, 29)\n",
      "    Selected 3 features\n",
      "    Fold accuracy: 0.9738\n",
      "  Fold 2/5\n",
      "    Applied SMOTE: (60852, 29) → (129261, 29)\n",
      "    Selected 3 features\n",
      "    Fold accuracy: 0.9736\n",
      "  Fold 3/5\n",
      "    Applied SMOTE: (60852, 29) → (129261, 29)\n",
      "    Selected 3 features\n",
      "    Fold accuracy: 0.9727\n",
      "  Fold 4/5\n",
      "    Applied SMOTE: (60852, 29) → (129261, 29)\n",
      "    Selected 3 features\n",
      "    Fold accuracy: 0.9732\n",
      "  Fold 5/5\n",
      "    Applied SMOTE: (60852, 29) → (129264, 29)\n",
      "    Selected 3 features\n",
      "    Fold accuracy: 0.9737\n",
      "  Random Forest - 5-Fold CV Results:\n",
      "  Mean Accuracy: 0.9734 (±0.0004)\n",
      "  Features selected in at least 3 folds: 3\n",
      "\n",
      "Evaluating XGBoost...\n",
      "  Fold 1/5\n",
      "    Applied SMOTE: (60852, 29) → (129261, 29)\n",
      "    Selected 2 features\n",
      "    Fold accuracy: 0.9738\n",
      "  Fold 2/5\n",
      "    Applied SMOTE: (60852, 29) → (129261, 29)\n",
      "    Selected 2 features\n",
      "    Fold accuracy: 0.9736\n",
      "  Fold 3/5\n",
      "    Applied SMOTE: (60852, 29) → (129261, 29)\n",
      "    Selected 2 features\n",
      "    Fold accuracy: 0.9727\n",
      "  Fold 4/5\n",
      "    Applied SMOTE: (60852, 29) → (129261, 29)\n",
      "    Selected 2 features\n",
      "    Fold accuracy: 0.9732\n",
      "  Fold 5/5\n",
      "    Applied SMOTE: (60852, 29) → (129264, 29)\n",
      "    Selected 2 features\n",
      "    Fold accuracy: 0.9737\n",
      "  XGBoost - 5-Fold CV Results:\n",
      "  Mean Accuracy: 0.9734 (±0.0004)\n",
      "  Features selected in at least 3 folds: 2\n",
      "\n",
      "Evaluating CatBoost...\n",
      "  Fold 1/5\n",
      "    Applied SMOTE: (60852, 29) → (129261, 29)\n",
      "    Selected 9 features\n",
      "    Fold accuracy: 0.9884\n",
      "  Fold 2/5\n",
      "    Applied SMOTE: (60852, 29) → (129261, 29)\n",
      "    Selected 9 features\n",
      "    Fold accuracy: 0.9856\n",
      "  Fold 3/5\n",
      "    Applied SMOTE: (60852, 29) → (129261, 29)\n",
      "    Selected 9 features\n",
      "    Fold accuracy: 0.9869\n",
      "  Fold 4/5\n",
      "    Applied SMOTE: (60852, 29) → (129261, 29)\n",
      "    Selected 9 features\n",
      "    Fold accuracy: 0.9857\n",
      "  Fold 5/5\n",
      "    Applied SMOTE: (60852, 29) → (129264, 29)\n",
      "    Selected 9 features\n",
      "    Fold accuracy: 0.9867\n",
      "  CatBoost - 5-Fold CV Results:\n",
      "  Mean Accuracy: 0.9867 (±0.0010)\n",
      "  Features selected in at least 3 folds: 9\n",
      "\n",
      "Evaluating MLPClassifier...\n",
      "  Fold 1/5\n",
      "    Applied SMOTE: (60852, 29) → (129261, 29)\n",
      "    Fold accuracy: 0.9895\n",
      "  Fold 2/5\n",
      "    Applied SMOTE: (60852, 29) → (129261, 29)\n",
      "    Fold accuracy: 0.9850\n",
      "  Fold 3/5\n",
      "    Applied SMOTE: (60852, 29) → (129261, 29)\n",
      "    Fold accuracy: 0.9875\n",
      "  Fold 4/5\n",
      "    Applied SMOTE: (60852, 29) → (129261, 29)\n",
      "    Fold accuracy: 0.9871\n",
      "  Fold 5/5\n",
      "    Applied SMOTE: (60852, 29) → (129264, 29)\n",
      "    Fold accuracy: 0.9887\n",
      "  MLPClassifier - 5-Fold CV Results:\n",
      "  Mean Accuracy: 0.9876 (±0.0015)\n",
      "\n",
      "==================================================\n",
      "14. SELECT BEST MODEL AND FEATURE SET\n",
      "==================================================\n",
      "Cross-validation results summary:\n",
      "           Model  Mean Accuracy   Std Dev\n",
      "3  MLPClassifier       0.987550  0.001532\n",
      "2       CatBoost       0.986682  0.001017\n",
      "0  Random Forest       0.973404  0.000435\n",
      "1        XGBoost       0.973404  0.000435\n",
      "\n",
      "Best model: MLPClassifier\n",
      "Using all 29 features for final model (MLP)\n",
      "\n",
      "==================================================\n",
      "12. HYPERPARAMETER TUNING\n",
      "==================================================\n",
      "Tuning hyperparameters for MLPClassifier...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Hyperparameter tuning completed in 636.43 seconds\n",
      "Best parameters: {'solver': 'adam', 'max_iter': 500, 'learning_rate': 'constant', 'hidden_layer_sizes': (50, 50, 50), 'alpha': 0.0001, 'activation': 'tanh'}\n",
      "Best CV accuracy: 0.9899\n",
      "\n",
      "==================================================\n",
      "15. TRAIN FINAL MODEL\n",
      "==================================================\n",
      "Applying SMOTE on full training set...\n",
      "Training data shape after SMOTE: (161577, 29)\n",
      "Training final MLPClassifier model...\n",
      "Final model training complete.\n",
      "\n",
      "==================================================\n",
      "16. MODEL EVALUATION ON TEST SET\n",
      "==================================================\n",
      "Test Accuracy: 0.9895\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class I       0.99      0.99      0.99      4022\n",
      "    Class II       1.00      0.99      0.99     13466\n",
      "   Class III       0.92      0.98      0.95      1529\n",
      "\n",
      "    accuracy                           0.99     19017\n",
      "   macro avg       0.97      0.99      0.98     19017\n",
      "weighted avg       0.99      0.99      0.99     19017\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n\" + \"*\"*80)\n",
    "print(\"RUNNING MODELING PIPELINE FOR BASELINE DATASET\")\n",
    "print(\"*\"*80)\n",
    "baseline_results = run_modeling_pipeline('baseline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da9e46eb-7b89-494d-9442-0bdbd2115782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************************************\n",
      "RUNNING MODELING PIPELINE FOR HYBRID DATASET\n",
      "********************************************************************************\n",
      "\n",
      "================================================================================\n",
      "    MODELING PIPELINE FOR HYBRID DATASET\n",
      "================================================================================\n",
      "\n",
      "\n",
      "==================================================\n",
      "1. EXPLORATORY DATA ANALYSIS\n",
      "==================================================\n",
      "Successfully loaded hybrid datasets:\n",
      "Training shape: (76065, 330)\n",
      "Testing shape: (19017, 330)\n",
      "\n",
      "Training Dataset Overview:\n",
      "Columns: ['Month_sin', 'Month_cos', 'Day_sin', 'Day_cos', 'DayOfWeek_sin', 'DayOfWeek_cos', 'Years_Since_First', 'Is_US', 'ProductClassification_Class II', 'ProductClassification_Class III', 'ProductType_Devices', 'ProductType_Drugs', 'ProductType_Food/Cosmetics', 'ProductType_Tobacco', 'ProductType_Veterinary', 'Status_Ongoing', 'Status_Terminated', 'Business_Structure_Association', 'Business_Structure_Company', 'Business_Structure_Corporation', 'Business_Structure_Inc', 'Business_Structure_LLC', 'Business_Structure_LLP', 'Business_Structure_LP', 'Business_Structure_Ltd', 'Business_Structure_Non-Profit', 'Business_Structure_Other', 'Business_Structure_PLC', 'Business_Structure_SA', 'text_svd_0', 'text_svd_1', 'text_svd_2', 'text_svd_3', 'text_svd_4', 'text_svd_5', 'text_svd_6', 'text_svd_7', 'text_svd_8', 'text_svd_9', 'text_svd_10', 'text_svd_11', 'text_svd_12', 'text_svd_13', 'text_svd_14', 'text_svd_15', 'text_svd_16', 'text_svd_17', 'text_svd_18', 'text_svd_19', 'text_svd_20', 'text_svd_21', 'text_svd_22', 'text_svd_23', 'text_svd_24', 'text_svd_25', 'text_svd_26', 'text_svd_27', 'text_svd_28', 'text_svd_29', 'text_svd_30', 'text_svd_31', 'text_svd_32', 'text_svd_33', 'text_svd_34', 'text_svd_35', 'text_svd_36', 'text_svd_37', 'text_svd_38', 'text_svd_39', 'text_svd_40', 'text_svd_41', 'text_svd_42', 'text_svd_43', 'text_svd_44', 'text_svd_45', 'text_svd_46', 'text_svd_47', 'text_svd_48', 'text_svd_49', 'text_svd_50', 'text_svd_51', 'text_svd_52', 'text_svd_53', 'text_svd_54', 'text_svd_55', 'text_svd_56', 'text_svd_57', 'text_svd_58', 'text_svd_59', 'text_svd_60', 'text_svd_61', 'text_svd_62', 'text_svd_63', 'text_svd_64', 'text_svd_65', 'text_svd_66', 'text_svd_67', 'text_svd_68', 'text_svd_69', 'text_svd_70', 'text_svd_71', 'text_svd_72', 'text_svd_73', 'text_svd_74', 'text_svd_75', 'text_svd_76', 'text_svd_77', 'text_svd_78', 'text_svd_79', 'text_svd_80', 'text_svd_81', 'text_svd_82', 'text_svd_83', 'text_svd_84', 'text_svd_85', 'text_svd_86', 'text_svd_87', 'text_svd_88', 'text_svd_89', 'text_svd_90', 'text_svd_91', 'text_svd_92', 'text_svd_93', 'text_svd_94', 'text_svd_95', 'text_svd_96', 'text_svd_97', 'text_svd_98', 'text_svd_99', 'text_svd_100', 'text_svd_101', 'text_svd_102', 'text_svd_103', 'text_svd_104', 'text_svd_105', 'text_svd_106', 'text_svd_107', 'text_svd_108', 'text_svd_109', 'text_svd_110', 'text_svd_111', 'text_svd_112', 'text_svd_113', 'text_svd_114', 'text_svd_115', 'text_svd_116', 'text_svd_117', 'text_svd_118', 'text_svd_119', 'text_svd_120', 'text_svd_121', 'text_svd_122', 'text_svd_123', 'text_svd_124', 'text_svd_125', 'text_svd_126', 'text_svd_127', 'text_svd_128', 'text_svd_129', 'text_svd_130', 'text_svd_131', 'text_svd_132', 'text_svd_133', 'text_svd_134', 'text_svd_135', 'text_svd_136', 'text_svd_137', 'text_svd_138', 'text_svd_139', 'text_svd_140', 'text_svd_141', 'text_svd_142', 'text_svd_143', 'text_svd_144', 'text_svd_145', 'text_svd_146', 'text_svd_147', 'text_svd_148', 'text_svd_149', 'text_svd_150', 'text_svd_151', 'text_svd_152', 'text_svd_153', 'text_svd_154', 'text_svd_155', 'text_svd_156', 'text_svd_157', 'text_svd_158', 'text_svd_159', 'text_svd_160', 'text_svd_161', 'text_svd_162', 'text_svd_163', 'text_svd_164', 'text_svd_165', 'text_svd_166', 'text_svd_167', 'text_svd_168', 'text_svd_169', 'text_svd_170', 'text_svd_171', 'text_svd_172', 'text_svd_173', 'text_svd_174', 'text_svd_175', 'text_svd_176', 'text_svd_177', 'text_svd_178', 'text_svd_179', 'text_svd_180', 'text_svd_181', 'text_svd_182', 'text_svd_183', 'text_svd_184', 'text_svd_185', 'text_svd_186', 'text_svd_187', 'text_svd_188', 'text_svd_189', 'text_svd_190', 'text_svd_191', 'text_svd_192', 'text_svd_193', 'text_svd_194', 'text_svd_195', 'text_svd_196', 'text_svd_197', 'text_svd_198', 'text_svd_199', 'text_svd_200', 'text_svd_201', 'text_svd_202', 'text_svd_203', 'text_svd_204', 'text_svd_205', 'text_svd_206', 'text_svd_207', 'text_svd_208', 'text_svd_209', 'text_svd_210', 'text_svd_211', 'text_svd_212', 'text_svd_213', 'text_svd_214', 'text_svd_215', 'text_svd_216', 'text_svd_217', 'text_svd_218', 'text_svd_219', 'text_svd_220', 'text_svd_221', 'text_svd_222', 'text_svd_223', 'text_svd_224', 'text_svd_225', 'text_svd_226', 'text_svd_227', 'text_svd_228', 'text_svd_229', 'text_svd_230', 'text_svd_231', 'text_svd_232', 'text_svd_233', 'text_svd_234', 'text_svd_235', 'text_svd_236', 'text_svd_237', 'text_svd_238', 'text_svd_239', 'text_svd_240', 'text_svd_241', 'text_svd_242', 'text_svd_243', 'text_svd_244', 'text_svd_245', 'text_svd_246', 'text_svd_247', 'text_svd_248', 'text_svd_249', 'text_svd_250', 'text_svd_251', 'text_svd_252', 'text_svd_253', 'text_svd_254', 'text_svd_255', 'text_svd_256', 'text_svd_257', 'text_svd_258', 'text_svd_259', 'text_svd_260', 'text_svd_261', 'text_svd_262', 'text_svd_263', 'text_svd_264', 'text_svd_265', 'text_svd_266', 'text_svd_267', 'text_svd_268', 'text_svd_269', 'text_svd_270', 'text_svd_271', 'text_svd_272', 'text_svd_273', 'text_svd_274', 'text_svd_275', 'text_svd_276', 'text_svd_277', 'text_svd_278', 'text_svd_279', 'text_svd_280', 'text_svd_281', 'text_svd_282', 'text_svd_283', 'text_svd_284', 'text_svd_285', 'text_svd_286', 'text_svd_287', 'text_svd_288', 'text_svd_289', 'text_svd_290', 'text_svd_291', 'text_svd_292', 'text_svd_293', 'text_svd_294', 'text_svd_295', 'text_svd_296', 'text_svd_297', 'text_svd_298', 'text_svd_299', 'Event Classification']\n",
      "\n",
      "Data Types:\n",
      "Month_sin               float64\n",
      "Month_cos               float64\n",
      "Day_sin                 float64\n",
      "Day_cos                 float64\n",
      "DayOfWeek_sin           float64\n",
      "                         ...   \n",
      "text_svd_296            float64\n",
      "text_svd_297            float64\n",
      "text_svd_298            float64\n",
      "text_svd_299            float64\n",
      "Event Classification     object\n",
      "Length: 330, dtype: object\n",
      "\n",
      "Target Distribution:\n",
      "       Class  Proportion\n",
      "0   Class II    0.708065\n",
      "1    Class I    0.211516\n",
      "2  Class III    0.080418\n",
      "\n",
      "==================================================\n",
      "2. DATA CLEANING VALIDATION\n",
      "==================================================\n",
      "\n",
      "Missing values in training data:\n",
      "No missing values found\n",
      "\n",
      "==================================================\n",
      "3. DATA PREPARATION\n",
      "==================================================\n",
      "Target Classes: ['Class I' 'Class II' 'Class III']\n",
      "\n",
      "==================================================\n",
      "4. TRAIN-TEST SPLIT VALIDATION\n",
      "==================================================\n",
      "Training set size: 76065\n",
      "Test set size: 19017\n",
      "Test set proportion: 20.00%\n",
      "\n",
      "Training class distribution:\n",
      "1    0.708065\n",
      "0    0.211516\n",
      "2    0.080418\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Test class distribution:\n",
      "1    0.708103\n",
      "0    0.211495\n",
      "2    0.080402\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "==================================================\n",
      "5. FEATURE ENGINEERING VALIDATION\n",
      "==================================================\n",
      "Number of features: 329\n",
      "Temporal features: 7\n",
      "Text-derived features: 300\n",
      "Categorical features: 21\n",
      "\n",
      "==================================================\n",
      "6. PRE-PROCESSED DATASET VALIDATION\n",
      "==================================================\n",
      "Checking for infinity or NaN values...\n",
      "No infinity or NaN values found.\n",
      "\n",
      "==================================================\n",
      "7. MODELING\n",
      "==================================================\n",
      "\n",
      "==================================================\n",
      "8-13. CROSS-VALIDATION WITH SMOTE & FEATURE SELECTION\n",
      "==================================================\n",
      "\n",
      "Evaluating Random Forest...\n",
      "  Fold 1/5\n",
      "    Applied SMOTE: (60852, 329) → (129261, 329)\n",
      "    Selected 43 features\n",
      "    Fold accuracy: 0.9895\n",
      "  Fold 2/5\n",
      "    Applied SMOTE: (60852, 329) → (129261, 329)\n",
      "    Selected 41 features\n",
      "    Fold accuracy: 0.9892\n",
      "  Fold 3/5\n",
      "    Applied SMOTE: (60852, 329) → (129261, 329)\n",
      "    Selected 43 features\n",
      "    Fold accuracy: 0.9892\n",
      "  Fold 4/5\n",
      "    Applied SMOTE: (60852, 329) → (129261, 329)\n",
      "    Selected 44 features\n",
      "    Fold accuracy: 0.9888\n",
      "  Fold 5/5\n",
      "    Applied SMOTE: (60852, 329) → (129264, 329)\n",
      "    Selected 42 features\n",
      "    Fold accuracy: 0.9895\n",
      "  Random Forest - 5-Fold CV Results:\n",
      "  Mean Accuracy: 0.9892 (±0.0003)\n",
      "  Features selected in at least 3 folds: 41\n",
      "\n",
      "Evaluating XGBoost...\n",
      "  Fold 1/5\n",
      "    Applied SMOTE: (60852, 329) → (129261, 329)\n",
      "    Selected 26 features\n",
      "    Fold accuracy: 0.9888\n",
      "  Fold 2/5\n",
      "    Applied SMOTE: (60852, 329) → (129261, 329)\n",
      "    Selected 30 features\n",
      "    Fold accuracy: 0.9888\n",
      "  Fold 3/5\n",
      "    Applied SMOTE: (60852, 329) → (129261, 329)\n",
      "    Selected 31 features\n",
      "    Fold accuracy: 0.9892\n",
      "  Fold 4/5\n",
      "    Applied SMOTE: (60852, 329) → (129261, 329)\n",
      "    Selected 29 features\n",
      "    Fold accuracy: 0.9882\n",
      "  Fold 5/5\n",
      "    Applied SMOTE: (60852, 329) → (129264, 329)\n",
      "    Selected 26 features\n",
      "    Fold accuracy: 0.9903\n",
      "  XGBoost - 5-Fold CV Results:\n",
      "  Mean Accuracy: 0.9890 (±0.0007)\n",
      "  Features selected in at least 3 folds: 26\n",
      "\n",
      "Evaluating CatBoost...\n",
      "  Fold 1/5\n",
      "    Applied SMOTE: (60852, 329) → (129261, 329)\n",
      "    Selected 28 features\n",
      "    Fold accuracy: 0.9926\n",
      "  Fold 2/5\n",
      "    Applied SMOTE: (60852, 329) → (129261, 329)\n",
      "    Selected 33 features\n",
      "    Fold accuracy: 0.9905\n",
      "  Fold 3/5\n",
      "    Applied SMOTE: (60852, 329) → (129261, 329)\n",
      "    Selected 29 features\n",
      "    Fold accuracy: 0.9922\n",
      "  Fold 4/5\n",
      "    Applied SMOTE: (60852, 329) → (129261, 329)\n",
      "    Selected 31 features\n",
      "    Fold accuracy: 0.9914\n",
      "  Fold 5/5\n",
      "    Applied SMOTE: (60852, 329) → (129264, 329)\n",
      "    Selected 34 features\n",
      "    Fold accuracy: 0.9922\n",
      "  CatBoost - 5-Fold CV Results:\n",
      "  Mean Accuracy: 0.9918 (±0.0008)\n",
      "  Features selected in at least 3 folds: 28\n",
      "\n",
      "Evaluating MLPClassifier...\n",
      "  Fold 1/5\n",
      "    Applied SMOTE: (60852, 329) → (129261, 329)\n",
      "    Fold accuracy: 0.9928\n",
      "  Fold 2/5\n",
      "    Applied SMOTE: (60852, 329) → (129261, 329)\n",
      "    Fold accuracy: 0.9920\n",
      "  Fold 3/5\n",
      "    Applied SMOTE: (60852, 329) → (129261, 329)\n",
      "    Fold accuracy: 0.9936\n",
      "  Fold 4/5\n",
      "    Applied SMOTE: (60852, 329) → (129261, 329)\n",
      "    Fold accuracy: 0.9929\n",
      "  Fold 5/5\n",
      "    Applied SMOTE: (60852, 329) → (129264, 329)\n",
      "    Fold accuracy: 0.9934\n",
      "  MLPClassifier - 5-Fold CV Results:\n",
      "  Mean Accuracy: 0.9930 (±0.0005)\n",
      "\n",
      "==================================================\n",
      "14. SELECT BEST MODEL AND FEATURE SET\n",
      "==================================================\n",
      "Cross-validation results summary:\n",
      "           Model  Mean Accuracy   Std Dev\n",
      "3  MLPClassifier       0.992953  0.000539\n",
      "2       CatBoost       0.991757  0.000750\n",
      "0  Random Forest       0.989233  0.000280\n",
      "1        XGBoost       0.989036  0.000702\n",
      "\n",
      "Best model: MLPClassifier\n",
      "Using all 329 features for final model (MLP)\n",
      "\n",
      "==================================================\n",
      "12. HYPERPARAMETER TUNING\n",
      "==================================================\n",
      "Tuning hyperparameters for MLPClassifier...\n",
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Hyperparameter tuning completed in 1284.95 seconds\n",
      "Best parameters: {'solver': 'adam', 'max_iter': 500, 'learning_rate': 'constant', 'hidden_layer_sizes': (100,), 'alpha': 0.0001, 'activation': 'tanh'}\n",
      "Best CV accuracy: 0.9937\n",
      "\n",
      "==================================================\n",
      "15. TRAIN FINAL MODEL\n",
      "==================================================\n",
      "Applying SMOTE on full training set...\n",
      "Training data shape after SMOTE: (161577, 329)\n",
      "Training final MLPClassifier model...\n",
      "Final model training complete.\n",
      "\n",
      "==================================================\n",
      "16. MODEL EVALUATION ON TEST SET\n",
      "==================================================\n",
      "Test Accuracy: 0.9924\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Class I       1.00      0.99      0.99      4022\n",
      "    Class II       1.00      0.99      0.99     13466\n",
      "   Class III       0.96      0.98      0.97      1529\n",
      "\n",
      "    accuracy                           0.99     19017\n",
      "   macro avg       0.98      0.99      0.99     19017\n",
      "weighted avg       0.99      0.99      0.99     19017\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\n\" + \"*\"*80)\n",
    "print(\"RUNNING MODELING PIPELINE FOR HYBRID DATASET\")\n",
    "print(\"*\"*80)\n",
    "hybrid_results = run_modeling_pipeline('hybrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fcfc705b-1dc8-48cd-85ad-2aa6c4dd7865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********************************************************************************\n",
      "COMPARING BASELINE VS HYBRID MODELS\n",
      "********************************************************************************\n",
      "\n",
      "Comparison Summary:\n",
      "               Metric       Baseline         Hybrid\n",
      "0          Model Type  MLPClassifier  MLPClassifier\n",
      "1         CV Accuracy         0.9899         0.9937\n",
      "2       Test Accuracy         0.9895         0.9924\n",
      "3  Number of Features             29            329\n",
      "\n",
      "Accuracy difference: 0.0029\n",
      "Percent improvement: 0.30%\n",
      "\n",
      "Conclusion: The hybrid model (with text features) performs better than the baseline model.\n",
      "\n",
      "Class-specific Performance Comparison:\n",
      "\n",
      "Class I Performance:\n",
      "  precision: Baseline=0.9943, Hybrid=0.9967, Diff=0.0025\n",
      "  recall: Baseline=0.9930, Hybrid=0.9903, Diff=-0.0027\n",
      "  f1-score: Baseline=0.9937, Hybrid=0.9935, Diff=-0.0001\n",
      "\n",
      "Class II Performance:\n",
      "  precision: Baseline=0.9960, Hybrid=0.9953, Diff=-0.0007\n",
      "  recall: Baseline=0.9896, Hybrid=0.9942, Diff=0.0046\n",
      "  f1-score: Baseline=0.9928, Hybrid=0.9948, Diff=0.0020\n",
      "\n",
      "Class III Performance:\n",
      "  precision: Baseline=0.9235, Hybrid=0.9567, Diff=0.0332\n",
      "  recall: Baseline=0.9791, Hybrid=0.9823, Diff=0.0033\n",
      "  f1-score: Baseline=0.9505, Hybrid=0.9693, Diff=0.0189\n",
      "\n",
      "Results saved. Analysis complete.\n"
     ]
    }
   ],
   "source": [
    "# Compare baseline and hybrid results\n",
    "if baseline_results and hybrid_results:\n",
    "    print(\"\\n\" + \"*\"*80)\n",
    "    print(\"COMPARING BASELINE VS HYBRID MODELS\")\n",
    "    print(\"*\"*80)\n",
    "    \n",
    "    # Create comparison table\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Metric': ['Model Type', 'CV Accuracy', 'Test Accuracy', 'Number of Features'],\n",
    "        'Baseline': [\n",
    "            baseline_results['best_model'],\n",
    "            f\"{baseline_results['cv_accuracy']:.4f}\",\n",
    "            f\"{baseline_results['test_accuracy']:.4f}\",\n",
    "            len(baseline_results['best_features'])\n",
    "        ],\n",
    "        'Hybrid': [\n",
    "            hybrid_results['best_model'],\n",
    "            f\"{hybrid_results['cv_accuracy']:.4f}\",\n",
    "            f\"{hybrid_results['test_accuracy']:.4f}\",\n",
    "            len(hybrid_results['best_features'])\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nComparison Summary:\")\n",
    "    print(comparison_df)\n",
    "    \n",
    "    # Calculate improvement\n",
    "    accuracy_diff = hybrid_results['test_accuracy'] - baseline_results['test_accuracy']\n",
    "    percent_improvement = (accuracy_diff / baseline_results['test_accuracy']) * 100\n",
    "    \n",
    "    print(f\"\\nAccuracy difference: {accuracy_diff:.4f}\")\n",
    "    print(f\"Percent improvement: {percent_improvement:.2f}%\")\n",
    "    \n",
    "    if accuracy_diff > 0:\n",
    "        print(\"\\nConclusion: The hybrid model (with text features) performs better than the baseline model.\")\n",
    "    elif accuracy_diff < 0:\n",
    "        print(\"\\nConclusion: The baseline model performs better than the hybrid model with text features.\")\n",
    "    else:\n",
    "        print(\"\\nConclusion: Both models perform similarly. Text features did not significantly impact performance.\")\n",
    "    \n",
    "    # Class-specific performance comparison\n",
    "    baseline_report = pd.DataFrame(baseline_results['classification_report'])\n",
    "    hybrid_report = pd.DataFrame(hybrid_results['classification_report'])\n",
    "    \n",
    "    print(\"\\nClass-specific Performance Comparison:\")\n",
    "    for class_name in baseline_report.columns:\n",
    "        if class_name not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "            print(f\"\\n{class_name} Performance:\")\n",
    "            for metric in ['precision', 'recall', 'f1-score']:\n",
    "                baseline_val = baseline_report.loc[metric, class_name]\n",
    "                hybrid_val = hybrid_report.loc[metric, class_name]\n",
    "                diff = hybrid_val - baseline_val\n",
    "                print(f\"  {metric}: Baseline={baseline_val:.4f}, Hybrid={hybrid_val:.4f}, Diff={diff:.4f}\")\n",
    "    \n",
    "    # Save comparison results\n",
    "    comparison_df.to_csv('results/baseline_vs_hybrid_comparison.csv', index=False)\n",
    "    \n",
    "    # Create performance comparison bar chart\n",
    "    metrics = ['CV Accuracy', 'Test Accuracy']\n",
    "    baseline_values = [baseline_results['cv_accuracy'], baseline_results['test_accuracy']]\n",
    "    hybrid_values = [hybrid_results['cv_accuracy'], hybrid_results['test_accuracy']]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    plt.bar(x - width/2, baseline_values, width, label='Baseline')\n",
    "    plt.bar(x + width/2, hybrid_values, width, label='Hybrid (with text)')\n",
    "    \n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Performance Comparison: Baseline vs Hybrid Models')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.legend()\n",
    "    plt.ylim(0.95, 1.0)  # Adjust as needed to highlight differences\n",
    "    \n",
    "    plt.savefig('results/baseline_vs_hybrid_accuracy_comparison.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"\\nResults saved. Analysis complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
